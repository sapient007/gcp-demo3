{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Demo 3: Text translation using AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gcpdemo3 import etl\n",
    "from gcpdemo3 import predict\n",
    "\n",
    "from google.cloud import automl_v1beta1 as automl\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get translation and AutoML credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set translation credentials\n",
    "trans_credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../credentials/translation/ml-sandbox-1-191918-b473cb40490b.json'\n",
    ")\n",
    "\n",
    "# set prediction credentials\n",
    "\n",
    "automl_credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../credentials/prediction/ml-sandbox-1-191918-58d6a5f3d5c2.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy raw book files from GCS\n",
    "os.system('gcloud config set project ml-sandbox-1-191918')\n",
    "os.mkdir('./temp')\n",
    "os.system('gsutil -m cp -r gs://gcp-cert-demo-3/opus ./temp')\n",
    "\n",
    "# process native english books and concatenate\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/en/Austen_Jane-Pride_and_Prejudice.xml',\n",
    "    out_path='./temp/Austen_Jane-Pride_and_Prejudice_en_processed.xml'\n",
    ")\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/en/Twain_Mark-Tom_Sawyer.xml',\n",
    "    out_path='./temp/Twain_Mark-Tom_Sawyer_en_processed.xml'\n",
    ")\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/en/Doyle_Arthur_Conan-Adventures_of_Sherlock_Holmes.xml',\n",
    "    out_path='./temp/Doyle_Arthur_Conan-Adventures_of_Sherlock_Holmes_en_processed.xml'\n",
    ")\n",
    "etl.concat_label_files(\n",
    "    in_paths=[\n",
    "        './temp/Austen_Jane-Pride_and_Prejudice_en_processed.xml',\n",
    "        './temp/Twain_Mark-Tom_Sawyer_en_processed.xml',\n",
    "        './temp/Doyle_Arthur_Conan-Adventures_of_Sherlock_Holmes_en_processed.xml'\n",
    "    ],\n",
    "    out_path='./temp/native.csv',\n",
    "    label='native'\n",
    ")\n",
    "\n",
    "# process professionally translated books and concatenate\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/en/Cervantes_Miguel-Don_Quijote.xml',\n",
    "    out_path='./temp/Cervantes_Miguel-Don_Quijote_en_processed.xml'\n",
    ")\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/en/Hugo_Victor-Notre_Dame_de_Paris.xml',\n",
    "    out_path='./temp/Hugo_Victor-Notre_Dame_de_Paris_en_processed.xml'\n",
    ")\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/en/Flaubert_Gustave-Madame_Bovary.xml',\n",
    "    out_path='./temp/Flaubert_Gustave-Madame_Bovary_en_processed.xml'\n",
    ")\n",
    "etl.concat_label_files(\n",
    "    in_paths=[\n",
    "        './temp/Cervantes_Miguel-Don_Quijote_en_processed.xml',\n",
    "        './temp/Hugo_Victor-Notre_Dame_de_Paris_en_processed.xml',\n",
    "        './temp/Flaubert_Gustave-Madame_Bovary_en_processed.xml'\n",
    "    ],\n",
    "    out_path='./temp/translated.csv',\n",
    "    label='translated'\n",
    ")\n",
    "\n",
    "# process and translate native spanish book\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/es/Cervantes_Miguel-Don_Quijote.xml',\n",
    "    out_path='./temp/Cervantes_Miguel-Don_Quijote_es_processed.xml'\n",
    ")\n",
    "etl.translate_book(\n",
    "    credentials=trans_credentials,\n",
    "    in_path='./temp/Cervantes_Miguel-Don_Quijote_es_processed.xml',\n",
    "    out_path='./temp/cervantes_translated.txt',\n",
    "    source='es',\n",
    "    target='en',\n",
    "    chunk_size=300\n",
    ")\n",
    "\n",
    "# process and translate native french book\n",
    "etl.process_book(\n",
    "    in_path='./temp/opus/Books/raw/fr/Hugo_Victor-Notre_Dame_de_Paris.xml',\n",
    "    out_path='./temp/Hugo_Victor-Notre_Dame_de_Paris_fr_processed.xml'\n",
    ")\n",
    "etl.translate_book(\n",
    "    credentials=trans_credentials,\n",
    "    in_path='./temp/Hugo_Victor-Notre_Dame_de_Paris_fr_processed.xml',\n",
    "    out_path='./temp/victorhugo_translated.txt',\n",
    "    source='fr',\n",
    "    target='en',\n",
    "    chunk_size=300\n",
    ")\n",
    "\n",
    "# concatenate translated books\n",
    "etl.concat_label_files(\n",
    "    in_paths=[\n",
    "        './temp/cervantes_translated.txt',\n",
    "        './temp/victorhugo_translated.txt'\n",
    "    ],\n",
    "    out_path='./temp/machine_translated.csv',\n",
    "    label='machine'\n",
    ")\n",
    "\n",
    "# concatenate files\n",
    "with open('./temp/train.csv', 'w+', encoding='utf8') as out_train:\n",
    "    with open('./temp/predict.csv', 'w+', encoding='utf8') as out_test:\n",
    "\n",
    "        # iterate through files and concatenate\n",
    "        in_paths = [\n",
    "            './temp/translated.csv',\n",
    "            './temp/native.csv',\n",
    "            './temp/machine_translated.csv'\n",
    "        ]\n",
    "        for in_path in in_paths:\n",
    "            with open(in_path, 'r', encoding='utf8') as in_file:\n",
    "                lines = in_file.readlines()\n",
    "                train_lines = lines[:-50]\n",
    "                test_lines = lines[-50:]\n",
    "                out_train.writelines(train_lines)\n",
    "                out_test.writelines(test_lines)\n",
    "                \n",
    "# upload to gcs\n",
    "timestamp = str(time.time()).split('.')[0]\n",
    "gs_train_path = f'gs://gcp-cert-demo-3/train_{timestamp}.csv'\n",
    "os.system(f'gsutil -m cp -r ./temp/train.csv {gs_train_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Auto ML Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client\n",
    "client = automl.AutoMlClient(credentials=automl_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A resource that represents Google Cloud Platform location.\n",
    "project_location = client.location_path('ml-sandbox-1-191918', 'us-central1')\n",
    "\n",
    "# Set dataset name and metadata.\n",
    "my_dataset = {\n",
    "    \"display_name\": f'demo3_dataset_{timestamp}',\n",
    "    \"text_classification_dataset_metadata\": {\"classification_type\": \"MULTICLASS\"},\n",
    "}\n",
    "\n",
    "# Create a dataset with the dataset metadata in the region.\n",
    "dataset = client.create_dataset(project_location, my_dataset)\n",
    "dataset_id = dataset.name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset (this process may take several minutes, check UI for completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full path of the dataset.\n",
    "dataset_full_id = client.dataset_path(\n",
    "    'ml-sandbox-1-191918', \n",
    "    'us-central1', \n",
    "    dataset_id\n",
    ")\n",
    "\n",
    "# Get the multiple Google Cloud Storage URIs.\n",
    "input_config = {\"gcs_source\": {\"input_uris\": [gs_train_path]}}\n",
    "\n",
    "# Import the dataset from the input URI.\n",
    "response = client.import_data(dataset_full_id, input_config)\n",
    "\n",
    "# synchronous check of operation status.\n",
    "print(\"Processing import...\")\n",
    "print(\"Data imported. {}\".format(response.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and deploy model once import complete (this process may take several minutes to hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A resource that represents Google Cloud Platform location.\n",
    "project_location = client.location_path(\n",
    "    'ml-sandbox-1-191918', \n",
    "    'us-central1'\n",
    ")\n",
    "\n",
    "# Set model name and model metadata for the dataset.\n",
    "timestamp = str(time.time()).split('.')[0]\n",
    "model_name = f'demo3_model_{timestamp}'\n",
    "my_model = {\n",
    "    \"display_name\": model_name,\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"text_classification_model_metadata\": {},\n",
    "}\n",
    "\n",
    "# Create a model with the model metadata in the region.\n",
    "response = client.create_model(project_location, my_model)\n",
    "print(\"Training operation name: {}\".format(response.operation.name))\n",
    "print(\"Training started...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(\n",
    "    './temp/predict.csv',\n",
    "    names=['line', 'label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new model (you will need to get the model ID from the AutoML UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model ID\n",
    "model_id = ''\n",
    "\n",
    "# make predictions using new deployed model\n",
    "predictor = predict.Predictor(\n",
    "    credentials=automl_credentials,\n",
    "    model_name=f'projects/261855689705/locations/us-central1/models/{model_id}'\n",
    ")\n",
    "predictions = predictor.predict(text_df)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using our deployed model\n",
    "predictor = predict.Predictor(\n",
    "    credentials=automl_credentials,\n",
    "    model_name='projects/261855689705/locations/us-central1/models/TCN7361261134285897728'\n",
    ")\n",
    "predictions = predictor.predict(text_df)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Temporary Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove temporary directory\n",
    "shutil.rmtree('./temp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
